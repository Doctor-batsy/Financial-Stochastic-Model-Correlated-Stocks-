{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version used: 2.13.0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Activation, Input\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "\n",
    "# Set data type\n",
    "#DTYPE='float32'\n",
    "DTYPE='float64'\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "print('TensorFlow version used: {}'.format(tf.__version__))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final time\n",
    "T = tf.constant(1., dtype=DTYPE)\n",
    "\n",
    "# Spatial dimensions\n",
    "dim = 100\n",
    "\n",
    "# Domain-of-interest at t=0\n",
    "a = 90 * tf.ones((dim), dtype=DTYPE)\n",
    "b = 110 * tf.ones((dim), dtype=DTYPE)\n",
    "\n",
    "# Interest rate\n",
    "r = tf.constant(1./20, dtype=DTYPE)\n",
    "\n",
    "# Drift\n",
    "mu = tf.constant(-1./20, dtype=DTYPE)\n",
    "\n",
    "# Strike price\n",
    "K = tf.constant(100., dtype=DTYPE)\n",
    "\n",
    "# Diffusion/volatility\n",
    "beta_i = 1./10 + 1./200*tf.range(1, dim+1, dtype=DTYPE)\n",
    "\n",
    "# Define terminal condition, i.e., payoff at maturity\n",
    "# Define the payoff function phi\n",
    "def phi(x):\n",
    "    return tf.exp(-mu * T) * tf.maximum(110 - tf.reduce_min(x, axis=1, keepdims=True), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a correlation matrix Q\n",
    "Q = np.ones([dim, dim]) * 0.5\n",
    "np.fill_diagonal(Q, 1.)\n",
    "\n",
    "# Perform Cholesky decomposition on Q to get sigma (volatility) and sigma_norms\n",
    "L = np.linalg.cholesky(Q)\n",
    "sigma_norms = tf.constant(np.linalg.norm(L, axis=0), dtype=DTYPE)\n",
    "sigma = tf.constant(L, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_H(num_samples, a, b):\n",
    "    \"\"\" Function to calculate H for num_samples many pairs of uniformly drawn starting\n",
    "    values X_0 and end points X_T of a stochastic process X with zero drift and\n",
    "    constant scalar diffusion. Starting points are drawn uniformly from the\n",
    "    hypercube [a,b] \\\\subset \\mathbb{R}^d. \"\"\"\n",
    "    # Create a 100x100 matrix with diagonal elements of 1 and off-diagonal elements of 1/2\n",
    " \n",
    "    dim = a.shape[0]\n",
    "    X0 = a + tf.random.uniform((num_samples, dim), dtype=DTYPE) * (b - a)\n",
    "    Xi = tf.random.normal(shape=(num_samples, dim), dtype=DTYPE)\n",
    "    XT = X0 * tf.exp((mu - 1/2 * (beta_i * sigma_norms) ** 2) * T +  tf.matmul(tf.sqrt(T)* Xi, sigma))\n",
    "    \n",
    "    # Stack the list of XT values along a new dimension to create the final output\n",
    "    \n",
    "    return tf.stack([X0, XT],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;241m+\u001b[39m expected_payoff \u001b[38;5;241m/\u001b[39m (b_size \u001b[38;5;241m*\u001b[39m mc_samples1)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mc_samples1):\n\u001b[1;32m---> 17\u001b[0m     Y \u001b[38;5;241m=\u001b[39m mc_samples2(Y)\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mmc_samples2\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m     10\u001b[0m Xi \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m(b_size, dim), dtype\u001b[38;5;241m=\u001b[39mDTYPE)\n\u001b[0;32m     11\u001b[0m upd \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp((mu \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (beta_i \u001b[38;5;241m*\u001b[39m sigma_norms) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m T \u001b[38;5;241m+\u001b[39m  tf\u001b[38;5;241m.\u001b[39mmatmul(tf\u001b[38;5;241m.\u001b[39msqrt(T)\u001b[38;5;241m*\u001b[39m Xi, sigma))\n\u001b[1;32m---> 12\u001b[0m XT \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(Xtest1, shape\u001b[38;5;241m=\u001b[39m[n_test1, dim, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(upd)\n\u001b[0;32m     13\u001b[0m expected_payoff \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39mreshape(phi(XT), [n_test1, b_size]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;241m+\u001b[39m expected_payoff \u001b[38;5;241m/\u001b[39m (b_size \u001b[38;5;241m*\u001b[39m mc_samples1)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1466\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1462\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1465\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[1;32m-> 1466\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(x, y, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1468\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1469\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1847\u001b[0m, in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1845\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor(y\u001b[38;5;241m.\u001b[39mindices, new_vals, y\u001b[38;5;241m.\u001b[39mdense_shape)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1847\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m multiply(x, y, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:530\u001b[0m, in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    482\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[0;32m    483\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    485\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m  For example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops\u001b[38;5;241m.\u001b[39mmul(x, y, name)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:7325\u001b[0m, in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   7323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   7324\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 7325\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m   7326\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMul\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, x, y)\n\u001b[0;32m   7327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   7328\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_test1 = 200000\n",
    "X1 = calculate_H(n_test1, a, b)\n",
    "Xtest1 = X1[:, :, 0]\n",
    "Xtest1 = tf.convert_to_tensor(Xtest1, dtype=DTYPE)\n",
    "Y = tf.zeros((Xtest1.shape[0], 1), dtype=DTYPE)  # Create an initial Ytest tensor filled with zeros\n",
    "b_size = 32\n",
    "mc_samples1 = 4000\n",
    "\n",
    "def mc_samples2(y):\n",
    "    Xi = tf.random.normal(shape=(b_size, dim), dtype=DTYPE)\n",
    "    upd = tf.exp((mu - 1/2 * (beta_i * sigma_norms) ** 2) * T +  tf.matmul(tf.sqrt(T)* Xi, sigma))\n",
    "    XT = tf.reshape(Xtest1, shape=[n_test1, dim, 1]) * tf.transpose(upd)\n",
    "    expected_payoff = tf.reduce_sum(tf.reshape(phi(XT), [n_test1, b_size]), axis=1, keepdims=True)\n",
    "    return y + expected_payoff / (b_size * mc_samples1)\n",
    "\n",
    "for i in range(mc_samples1):\n",
    "    Y = mc_samples2(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(dim, activation='tanh',\n",
    "               num_hidden_neurons=200,\n",
    "               num_hidden_layers=3,\n",
    "               initializer=GlorotUniform()):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(dim,)))\n",
    "    model.add(BatchNormalization(epsilon=1e-6))\n",
    "\n",
    "    # Create a fixed number of hidden layers\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(num_hidden_neurons,\n",
    "                        activation=None,\n",
    "                        use_bias=False,\n",
    "                        kernel_initializer=initializer))\n",
    "        model.add(BatchNormalization(epsilon=1e-6))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "    model.add(Dense(1,\n",
    "                    activation=None,\n",
    "                    use_bias=False,\n",
    "                    kernel_initializer=initializer))\n",
    "    model.add(BatchNormalization(epsilon=1e-6))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_5 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 200)               20000     \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 200)               800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 200)               0         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_5 (Dense)             (None, 200)               40000     \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 200)               800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 200)               40000     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 200)               800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 200       \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 1)                 4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103004 (804.72 KB)\n",
      "Trainable params: 101602 (793.77 KB)\n",
      "Non-trainable params: 1402 (10.95 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = init_model(dim=dim)\n",
    "\n",
    "# Learning rate schedule and optimizer\n",
    "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([750, 1500], [1e-3, 1e-4, 1e-5])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-8)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(Y)\n",
    "df = pd.DataFrame(Xtest1)\n",
    "df['Y'] = Y\n",
    "X_train_val1, X_test1, y_train_val1, y_test1 = train_test_split(df.drop(columns=['Y']), df.loc[:, 'Y'], test_size=0.2, random_state=42)\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import time\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_times = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_times.clear()\n",
    "        self.train_losses.clear()\n",
    "        self.val_losses.clear()\n",
    "        self.predictions.clear()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "\n",
    "        # Save the losses\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "        # Save predictions for the validation data\n",
    "        predictions = self.model.predict(X_val1).flatten()  # Make sure X_val1 is defined and accessible\n",
    "        self.predictions.append(predictions)\n",
    "\n",
    "# Initialize the callback\n",
    "custom_callback = CustomCallback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 2s 3ms/step\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 2s 3ms/step\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Total time for model fitting: 0.011 hours\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the custom callback\n",
    "start_time = time.time()\n",
    "history2 = model.fit(\n",
    "    X_train_val1, \n",
    "    y_train_val1,\n",
    "    batch_size=256, \n",
    "    epochs=3000, \n",
    "    validation_data=(X_val1, y_val1), \n",
    "    callbacks=[custom_callback]  # Add custom_callback here\n",
    ")\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Print the total time for model fitting\n",
    "print(f\"Total time for model fitting: {total_time/3600:.3f} hours\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last prediction from each epoch\n",
    "last_predictions = [epoch_predictions[-1] for epoch_predictions in custom_callback.predictions]\n",
    "\n",
    "# Create a DataFrame with the logged data\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': range(1, len(custom_callback.train_losses) + 1),\n",
    "    'Training_Loss': custom_callback.train_losses,\n",
    "    'Validation_Loss': custom_callback.val_losses,\n",
    "    'Epoch_Run_Time(s)': custom_callback.epoch_times,\n",
    "    'Last_Prediction': last_predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('model_performance_GBM_corrolated_noise.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Accuracy on Validation Data: -4050.8979273003847\n",
      "Accuracy on Test Data: -2175.4368407440584\n",
      "Accuracy on Train Data: -2175.4368407440584\n",
      "Mean Absolute Error (MAE) on Validation Data: 0.9333\n",
      "Mean Absolute Error (MAE) on Test Data: 1.0469\n",
      "Mean Absolute Error (MAE) on Training Data: 1.1560\n",
      "Mean Squared Error (MSE) on Validation Data: 0.8935\n",
      "Mean Squared Error (MSE) on Test Data: 1.1486\n",
      "Mean Squared Error (MSE) on Training Data: 1.4507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming you have already trained your model and obtained predictions for both validation and test data\n",
    "y_pred_val1 = model.predict(X_val1)  # Predictions for validation data\n",
    "y_pred_test1 = model.predict(X_test1)  # Predictions for test data\n",
    "y_pred_train1 = model.predict(X_train1)  # Predictions for training data\n",
    "\n",
    "\n",
    "# Calculate R-squared for both sets\n",
    "r_squared_val1 = r2_score(y_val1, y_pred_val1)\n",
    "r_squared_test1 = r2_score(y_test1, y_pred_test1)\n",
    "r_squared_train1 = r2_score(y_train1, y_pred_train1)\n",
    "\n",
    "# Calculate MAE for all three datasets\n",
    "mae_val1 = mean_absolute_error(y_val1, y_pred_val1)\n",
    "mae_test1 = mean_absolute_error(y_test1, y_pred_test1)\n",
    "mae_train1 = mean_absolute_error(y_train1, y_pred_train1)\n",
    "\n",
    "# Calculate MSE for all three datasets\n",
    "mse_val1 = mean_squared_error(y_val1, y_pred_val1)\n",
    "mse_test1 = mean_squared_error(y_test1, y_pred_test1)\n",
    "mse_train1 = mean_squared_error(y_train1, y_pred_train1)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Accuracy on Validation Data: {r_squared_val1 * 100}\")\n",
    "print(f\"Accuracy on Test Data: {r_squared_test1 * 100}\")\n",
    "print(f\"Accuracy on Train Data: {r_squared_test1 * 100}\")\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE) on Validation Data: {mae_val1:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Test Data: {mae_test1:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Training Data: {mae_train1:.4f}\")\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Validation Data: {mse_val1:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE) on Test Data: {mse_test1:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE) on Training Data: {mse_train1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "performance = pd.read_csv(\"model_performance_GBM_corrolated_noise.csv\")\n",
    "# Create a subplot grid with two subplots side by side\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Plot the blue line for \"Approximation\" and the horizontal line for \"Exact Solution\" on the first subplot\n",
    "plt.plot(performance['Training_Loss'], color = 'b',label=\"Training_Loss\", linewidth=0.7)\n",
    "plt.plot(performance['Validation_Loss'], color = 'r',label=\"Validation_Loss\", linewidth=0.7)\n",
    "\n",
    "plt.xlabel('Epochs',fontsize=\"23\")\n",
    "plt.ylabel('Loss',fontsize=\"23\")\n",
    "plt.title('Training and Validation Loss',fontsize=\"23\")\n",
    "plt.xticks(size = 20)\n",
    "plt.yticks(size = 20)\n",
    "\n",
    "\n",
    "# Customize the plot style\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.legend(fontsize=\"21\")\n",
    "plt.ylim(-0.3,3)\n",
    "# plt.savefig(\"training_loss_plot_multi assets.jpg\", format=\"jpg\", dpi = 500)\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(performance['Prediction'], color = 'b',label=\"Predicted Values\", linewidth=0.9)\n",
    "plt.axhline(y=np.mean(Y), color='r',label=\"Target Value\", linestyle='--',linewidth=1)\n",
    "plt.legend(fontsize=\"20\",loc=\"lower right\")\n",
    "plt.ylim(100,103.5)\n",
    "plt.xlabel('Epochs',fontsize=\"20\")\n",
    "plt.ylabel('Option Values',fontsize=\"20\")\n",
    "plt.title('Predicted Values',fontsize=\"23\")\n",
    "plt.xticks(size = 20)\n",
    "plt.yticks(size = 20)\n",
    "# plt.savefig(\"solution_GBM_corrolated_noise.jpg\", format=\"jpeg\", dpi = 500)\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
